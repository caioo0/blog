<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>深度学习推荐系统作业：deepFM介绍和代码实现</title>
    <url>/2021/03/22/DeepFM/</url>
    <content><![CDATA[<h2 id="阅读前思考"><a href="#阅读前思考" class="headerlink" title="阅读前思考"></a>阅读前思考</h2><blockquote>
<ol>
<li>如果对于FM采用随机梯度下降SGD训练模型参数，请写出模型各个参数的梯度和FM参数训练的复杂度</li>
<li>对于下图(3-1)所示，根据你的理解Sparse Feature中的不同颜色节点分别表示什么意思</li>
</ol>
</blockquote>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>对于CTR预估的模型来说，一个很重要的点就是学习用户行为对应的特征背后的潜在联系。虽然目前在这个领域已经取得了一些进展（截止2017年），但是目前的做法要么在低维或者高维的特征上存在很大的偏差，要么需要大量的专家级的特征工程。</p>
<p>哈工大和华为联合设计了一种新的模型DeepFM，从而找到了一种可能性，可以同时提升低维和高维的特征。它结合了FM和神经网络模型的长处，和Google最新的Wide &amp; Deep模型的做法相比，取得了更大的进步，并且还免去了特征工程的部分。<br>论文详见：<a href="https://arxiv.org/abs/1703.04247" target="_blank" rel="noopener">A Factorization-Machine based Neural Network for CTR Prediction</a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><strong>DeepFM模型本质</strong></p>
<ol>
<li>将Wide &amp; Deep 部分的wide部分由 人工特征工程+LR 转换为FM模型，避开了人工特征工程；</li>
<li>FM模型与deep part共享feature embedding。</li>
</ol>
<p><strong>为什么要用FM代替线性部分（wide）呢？</strong></p>
<p>因为线性模型有个致命的缺点：无法提取高阶的组合特征。<br>FM通过隐向量latent vector做内积来表示组合特征，从理论上解决了低阶和高阶组合特征提取的问题。但是实际应用中受限于计算复杂度，一般也就只考虑到2阶交叉特征。</p>
<p><strong>各模型间的对比</strong> </p>
<ol>
<li>随着DNN在图像、语音、NLP等领域取得突破，人们意识到DNN在特征表示上的天然优势。相继提出了使用CNN或RNN来做CTR预估的模型,但是</li>
</ol>
<blockquote>
<p>CNN模型缺点：偏向于学习相邻特征的组合特征。<br>RNN模型缺点：比较适用于有序列(时序)关系的数据。</p>
</blockquote>
<ol>
<li>FNN (Factorization-machine supported Neural Network) 的提出，应该算是一次非常不错的尝试：先使用预先训练好的FM，得到隐向量，然后作为DNN的输入来训练模型。缺点在于：受限于FM预训练的效果。</li>
<li>PNN (Product-based Neural Network)，PNN为了捕获高阶组合特征，在embedding layer和first hidden layer之间增加了一个product layer。根据product layer使用内积、外积、混合分别衍生出IPNN, OPNN, PNN*三种类型。</li>
</ol>
<p>无论是FNN还是PNN，他们都有一个绕不过去的缺点：<strong>对于低阶的组合特征，学习到的比较少。</strong>而前面我们说过，低阶特征对于CTR也是非常重要的。</p>
<ol>
<li>为了同时学习低阶和高阶组合特征，Google提出了Wide&amp;Deep模型。它混合了一个线性模型（Wide part）和Deep模型(Deep part)。这两部分模型需要不同的输入，而Wide part部分的输入，依旧依赖人工特征工程。</li>
</ol>
<p><strong>DeepFM优势</strong> </p>
<p>上面这些模型普遍都存在两个问题：</p>
<ol>
<li>偏向于提取低阶或者高阶的组合特征。</li>
<li>不能同时提取这两种类型的特征。</li>
<li>需要专业的领域知识来做特征工程。</li>
</ol>
<p>DeepFM可以看做是从FM基础上衍生的算法，将Deep与FM相结合，用FM做特征间低阶组合，用Deep NN部分做特征间高阶组合，通过并行的方式组合两种方法，使得最终的架构具有以下特点。</p>
<ol>
<li>不需要预训练 FM 得到隐向量；</li>
<li>不需要人工特征工程；</li>
<li>能同时学习低阶和高阶的组合特征；</li>
<li>FM 模块和 Deep 模块共享 Feature Embedding 部分，可以更快的训练，以及更精确的训练学习。</li>
</ol>
<p>————————————————</p>
<h2 id="DeepFM模型介绍"><a href="#DeepFM模型介绍" class="headerlink" title="DeepFM模型介绍"></a>DeepFM模型介绍</h2><h4 id="DeepFM的模型结构："><a href="#DeepFM的模型结构：" class="headerlink" title="DeepFM的模型结构："></a>DeepFM的模型结构：</h4><p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225180556628.png" alt="image-20210225180556628" style="zoom:50%;"></p>
<p>DeepFM包含两部分：神经网络部分与因子分解机部分，分别负责低阶特征的提取和高阶特征的提取。这两部分共享同样的输入。DeepFM的预测结果可以写为：$$\hat{y}=sigmoid(y_{fm} + y_{dnn})$$</p>
<h3 id="FM部分"><a href="#FM部分" class="headerlink" title="FM部分"></a>FM部分</h3><p>详细内容参考FM模型部分的内容，下图是FM的一个结构图，从图中大致可以看出FM Layer是由一阶特征和二阶特征Concatenate到一起在经过一个Sigmoid得到logits（结合FM的公式一起看），所以在实现的时候需要单独考虑linear部分和FM交叉特征部分。<br>$$\hat{y}_{FM}(x) = w_0+\sum_{i=1}^N w_ix_i + \sum_{i=1}^N \sum_{j=i+1}^N v_i^T v_j x_ix_j$$<br><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181340313.png" alt="image-20210225181340313" style="zoom: 67%;"></p>
<h3 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a>Deep部分</h3><p>Deep架构图</p>
<p><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20210225181010107.png" alt="image-20210225181010107" style="zoom:50%;"></p>
<p>Deep Module是为了学习高阶的特征组合，在上图中使用用全连接的方式将Dense Embedding输入到Hidden Layer，这里面Dense Embeddings就是为了解决DNN中的参数爆炸问题，这也是推荐模型中常用的处理方法。</p>
<p>Embedding层的输出是将所有id类特征对应的embedding向量concat到到一起输入到DNN中。其中$v_i$表示第i个field的embedding，m是field的数量。<br>$$z_1=[v_1, v_2, …, v_m]$$<br>上一层的输出作为下一层的输入，我们得到：<br>$$z_L=\sigma(W_{L-1} z_{L-1}+b_{L-1})$$<br>其中$\sigma$表示激活函数，$z, W, b $分别表示该层的输入、权重和偏置。</p>
<p>最后进入DNN部分输出使用sigmod激活函数进行激活：</p>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><p>完整数据集:<a href="https://labs.criteo.com/2014/02/download-kaggle-display-advertising-challenge-dataset/" target="_blank" rel="noopener">crite</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:Jo Choi</span></span><br><span class="line"><span class="comment"># Date:2021-03-21</span></span><br><span class="line"><span class="comment"># Email:cai_oo@sina.com.cn</span></span><br><span class="line"><span class="comment"># Blog: caioo0.github.io</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：criteo_sample</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">----------------------------</span></span><br><span class="line"><span class="string">ETA: 0s - loss: 0.6558 - binary_crossentropy: 0.6558 - auc: 0.569 - 0s 28ms/step - loss: 0.6664 - binary_crossentropy: 0.6664 - </span></span><br><span class="line"><span class="string">auc: 0.6306 - val_loss: 0.7323 - val_binary_crossentropy: 0.7323 - val_auc: 0.6724</span></span><br><span class="line"><span class="string">----------------------------</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,LabelEncoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> SparseFeat, DenseFeat, VarLenSparseFeat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_process</span><span class="params">(data_df,dense_features,sparse_features)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    数据预处理，包括填充缺失值，数值处理，类别编码</span></span><br><span class="line"><span class="string">    :param data_df: Data_Frame格式的数据</span></span><br><span class="line"><span class="string">    :param dense_features: 数值特征名称列表</span></span><br><span class="line"><span class="string">    :param sparse_features: 离散特征名称列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">     <span class="comment">#数值型特征缺失值填充0.0</span></span><br><span class="line">    data_df[dense_features] = data_df[dense_features].fillna(<span class="number">0.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> dense_features:</span><br><span class="line">        data_df[f] = data_df[f].apply(<span class="keyword">lambda</span> x: np.log(x + <span class="number">1</span>) <span class="keyword">if</span> x &gt; <span class="number">-1</span> <span class="keyword">else</span> <span class="number">-1</span>)</span><br><span class="line">     </span><br><span class="line">    <span class="comment">#离散型特征缺失值填充-1   </span></span><br><span class="line">    data_df[sparse_features] = data_df[sparse_features].fillna(<span class="string">"-1"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> sparse_features:</span><br><span class="line">        <span class="comment">#标准化</span></span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data_df[f] = lbe.fit_transform(data_df[f])</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#返回    </span></span><br><span class="line">    <span class="keyword">return</span> data_df[dense_features + sparse_features]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_input_layers</span><span class="params">(feature_columns)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    构建输入层</span></span><br><span class="line"><span class="string">    :param feature_columns : 数据集中的所有特征对应的特征标记</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建input 层字典，并以dense 和 sparse 两类字典的形式返回</span></span><br><span class="line">    dense_input_dict,sparse_input_dict = &#123;&#125; ,&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> feature_columns:</span><br><span class="line">        <span class="keyword">if</span> isinstance(fc,SparseFeat):</span><br><span class="line">            sparse_input_dict[fc.name] = Input(shape = (<span class="number">1</span>,), name = fc.name)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(fc,DenseFeat):</span><br><span class="line">            dense_input_dict[fc.name] = Input(shape = (fc.dimension, ), name = fc.name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dense_input_dict, sparse_input_dict</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding_layers</span><span class="params">(feature_columns, input_layers_dict, is_linear)</span>:</span></span><br><span class="line">    <span class="comment"># 定义一个embedding层对应的字典</span></span><br><span class="line">    embedding_layers_dict = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), feature_columns)) <span class="keyword">if</span> feature_columns <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度</span></span><br><span class="line">    <span class="keyword">if</span> is_linear:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size , <span class="number">1</span>, name = <span class="string">'1d_emb_'</span> + fc.name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size , fc.embedding_dim , name = <span class="string">'kd_emb_'</span> + fc.name) </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> embedding_layers_dict </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_linear_logits</span><span class="params">(dense_input_dict, sparse_input_dict, sparse_feature_columns)</span>:</span></span><br><span class="line">    <span class="comment"># 将所有的dense特征的Input层，然后经过一个全连接层得到dense特征的logits</span></span><br><span class="line">    concat_dense_inputs = Concatenate(axis=<span class="number">1</span>)(list(dense_input_dict.values()))</span><br><span class="line">    dense_logits_output = Dense(<span class="number">1</span>)(concat_dense_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：</span></span><br><span class="line">    <span class="comment"># 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢</span></span><br><span class="line">    <span class="comment"># 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高</span></span><br><span class="line">    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应</span></span><br><span class="line">    sparse_1d_embed = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        feat_input = sparse_input_dict[fc.name]</span><br><span class="line">        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) <span class="comment"># B x 1</span></span><br><span class="line">        sparse_1d_embed.append(embed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接</span></span><br><span class="line">    <span class="comment"># 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)</span></span><br><span class="line">    sparse_logits_output = Add()(sparse_1d_embed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits</span></span><br><span class="line">    linear_logits = Add()([dense_logits_output, sparse_logits_output])</span><br><span class="line">    <span class="keyword">return</span> linear_logits</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FM_Layer</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(FM_Layer, self).__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        优化后的公式： 0.5 * 求和（和的平方 - 平方的和）  =&gt;&gt; B x 1</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># B x n x K </span></span><br><span class="line">        concated_embeds_value = inputs </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># B x 1 x k </span></span><br><span class="line">        square_of_sum = tf.square(tf.reduce_sum(concated_embeds_value, axis = <span class="number">1</span>,keepdims = <span class="keyword">True</span> ))</span><br><span class="line">        sum_of_square = tf.reduce_sum(concated_embeds_value * concated_embeds_value, axis = <span class="number">1</span>,keepdims = <span class="keyword">True</span>)</span><br><span class="line">        cross_term = square_of_sum - sum_of_square </span><br><span class="line">        <span class="comment"># B x 1</span></span><br><span class="line">        cross_term = <span class="number">0.5</span> * tf.reduce_sum(cross_term, axis = <span class="number">2</span> ,keepdims = <span class="keyword">False</span>) </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cross_term</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">None</span>, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fm_logits</span><span class="params">(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 讲特征中的squarse特征筛选出来</span></span><br><span class="line">    sqarse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), sparse_feature_columns))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 只考虑sparse的二阶交叉，将所有的embedding拼接到一起进行FM计算</span></span><br><span class="line">    <span class="comment"># 因为类别型数据输入的只有0和1所以不需要考虑将隐向量与X相乘， 直接对隐向量进行操作即可</span></span><br><span class="line">    sparse_kd_embed = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        feat_input = sparse_input_dict[fc.name]</span><br><span class="line">        _embed = dnn_embedding_layers[fc.name](feat_input) <span class="comment"># B x 1 x k</span></span><br><span class="line">        sparse_kd_embed.append(_embed)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 将所有sparse的embedding拼接起来，得到( n, k)的矩阵， 其中n为特征数，k为embedding大小</span></span><br><span class="line">    <span class="comment"># B x n x k</span></span><br><span class="line">    concat_sparse_kd_embed = Concatenate(axis = <span class="number">1</span>)(sparse_kd_embed) </span><br><span class="line">    fm_cross_out = FM_Layer()(concat_sparse_kd_embed)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> fm_cross_out </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dnn_logits</span><span class="params">(sparse_input_dict, sparse_feature_columns, dnn_embedding_layers)</span>:</span></span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), sparse_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有非零的sparse特征对应的embedding拼接到一起</span></span><br><span class="line">    sparse_kd_embed = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        feat_input = sparse_input_dict[fc.name]</span><br><span class="line">        _embed = dnn_embedding_layers[fc.name](feat_input) <span class="comment"># B x 1 x k</span></span><br><span class="line">        _embed = Flatten()(_embed) <span class="comment"># B x k</span></span><br><span class="line">        sparse_kd_embed.append(_embed)</span><br><span class="line"></span><br><span class="line">    concat_sparse_kd_embed = Concatenate(axis=<span class="number">1</span>)(sparse_kd_embed) <span class="comment"># B x nk   </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dnn层，这里的Dropout参数，Dense中的参数都可以自己设定，以及Dense的层数都可以自行设定</span></span><br><span class="line">    mlp_out = Dropout(<span class="number">0.5</span>)(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)(concat_sparse_kd_embed))  </span><br><span class="line">    mlp_out = Dropout(<span class="number">0.3</span>)(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)(mlp_out))</span><br><span class="line">    mlp_out = Dropout(<span class="number">0.1</span>)(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)(mlp_out))</span><br><span class="line"></span><br><span class="line">    dnn_out = Dense(<span class="number">1</span>)(mlp_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dnn_out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">DeepFM</span><span class="params">(linear_feature_columns, dnn_feature_columns)</span>:</span></span><br><span class="line">    <span class="comment"># 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding</span></span><br><span class="line">    linear_sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), linear_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span></span><br><span class="line">    <span class="comment"># 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span></span><br><span class="line">    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># linear_logits由两部分组成，分别是dense特征的logits和sparse特征的logits</span></span><br><span class="line">    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span></span><br><span class="line">    <span class="comment"># embedding层用户构建FM交叉部分和DNN的输入部分</span></span><br><span class="line">    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入到dnn中的所有sparse特征筛选出来</span></span><br><span class="line">    dnn_sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), dnn_feature_columns))</span><br><span class="line"></span><br><span class="line">    fm_logits = get_fm_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers) <span class="comment"># 只考虑二阶项</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将所有的Embedding都拼起来，一起输入到dnn中</span></span><br><span class="line">    dnn_logits = get_dnn_logits(sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将linear,FM,dnn的logits相加作为最终的logits</span></span><br><span class="line">    output_logits = Add()([linear_logits, fm_logits, dnn_logits])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的激活函数使用sigmoid</span></span><br><span class="line">    output_layers = Activation(<span class="string">"sigmoid"</span>)(output_logits)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layers)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">'./data/criteo_sample.txt'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 划分dense和sparse特征</span></span><br><span class="line">    columns = data.columns.values</span><br><span class="line">    dense_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'I'</span> <span class="keyword">in</span> feat]</span><br><span class="line">    sparse_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'C'</span> <span class="keyword">in</span> feat]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 简单的数据预处理</span></span><br><span class="line">    train_data = data_process(data, dense_features, sparse_features)</span><br><span class="line">    train_data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>]</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）</span></span><br><span class="line">    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 构建DeepFM模型</span></span><br><span class="line">    history = DeepFM(linear_feature_columns, dnn_feature_columns)</span><br><span class="line">    history.summary()</span><br><span class="line">    history.compile(optimizer=<span class="string">"adam"</span>, </span><br><span class="line">                loss=<span class="string">"binary_crossentropy"</span>, </span><br><span class="line">                metrics=[<span class="string">"binary_crossentropy"</span>, tf.keras.metrics.AUC(name=<span class="string">'auc'</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入数据转化成字典的形式输入</span></span><br><span class="line">    train_model_input = &#123;name: data[name] <span class="keyword">for</span> name <span class="keyword">in</span> dense_features + sparse_features&#125;</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    history.fit(train_model_input, train_data[<span class="string">'label'</span>].values,</span><br><span class="line">            batch_size=<span class="number">64</span>, epochs=<span class="number">8</span>, validation_split=<span class="number">0.2</span>, )</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)             (None, 256)          0           dense_4[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">add_3 (Add)                     (None, 1)            0           dense_1[0][0]                    </span><br><span class="line">                                                                 add_2[0][0]                      </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">fm__layer (FM_Layer)            (None, 1)            0           concatenate_2[0][0]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_5 (Dense)                 (None, 1)            257         dropout_2[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">add_4 (Add)                     (None, 1)            0           add_3[0][0]                      </span><br><span class="line">                                                                 fm__layer[0][0]                  </span><br><span class="line">                                                                 dense_5[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation (Activation)         (None, 1)            0           add_4[0][0]                      </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 170,125</span><br><span class="line">Trainable params: 170,125</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Epoch 1/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.8385 - binary_crossentropy: 0.8385 - auc: 0.453 - 1s 332ms/step - loss: 1.0251 - binary_crossentropy: 1.0251 - auc: 0.5025 - val_loss: 1.1393 - val_binary_crossentropy: 1.1393 - val_auc: 0.6481</span><br><span class="line">Epoch 2/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.9249 - binary_crossentropy: 0.9249 - auc: 0.445 - 0s 26ms/step - loss: 0.9782 - binary_crossentropy: 0.9782 - auc: 0.5065 - val_loss: 1.0822 - val_binary_crossentropy: 1.0822 - val_auc: 0.6368</span><br><span class="line">Epoch 3/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.6185 - binary_crossentropy: 0.6185 - auc: 0.608 - 0s 28ms/step - loss: 0.9260 - binary_crossentropy: 0.9260 - auc: 0.5326 - val_loss: 1.0137 - val_binary_crossentropy: 1.0137 - val_auc: 0.6453</span><br><span class="line">Epoch 4/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.7218 - binary_crossentropy: 0.7218 - auc: 0.606 - 0s 29ms/step - loss: 0.8665 - binary_crossentropy: 0.8665 - auc: 0.5416 - val_loss: 0.9309 - val_binary_crossentropy: 0.9309 - val_auc: 0.6467</span><br><span class="line">Epoch 5/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.4631 - binary_crossentropy: 0.4631 - auc: 0.700 - 0s 24ms/step - loss: 0.7977 - binary_crossentropy: 0.7977 - auc: 0.5513 - val_loss: 0.8414 - val_binary_crossentropy: 0.8414 - val_auc: 0.6496</span><br><span class="line">Epoch 6/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.9847 - binary_crossentropy: 0.9847 - auc: 0.415 - 0s 23ms/step - loss: 0.7581 - binary_crossentropy: 0.7581 - auc: 0.5584 - val_loss: 0.7629 - val_binary_crossentropy: 0.7629 - val_auc: 0.6581</span><br><span class="line">Epoch 7/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.7810 - binary_crossentropy: 0.7810 - auc: 0.591 - 0s 23ms/step - loss: 0.6955 - binary_crossentropy: 0.6955 - auc: 0.5933 - val_loss: 0.7338 - val_binary_crossentropy: 0.7338 - val_auc: 0.6681</span><br><span class="line">Epoch 8/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.6558 - binary_crossentropy: 0.6558 - auc: 0.569 - 0s 28ms/step - loss: 0.6664 - binary_crossentropy: 0.6664 - auc: 0.6306 - val_loss: 0.7323 - val_binary_crossentropy: 0.7323 - val_auc: 0.6724</span><br></pre></td></tr></table></figure>
<h3 id="思考题解答"><a href="#思考题解答" class="headerlink" title="思考题解答"></a>思考题解答</h3><p><strong>1. 如果对于FM采用随机梯度下降SGD训练模型参数，请写出模型各个参数的梯度和FM参数训练的复杂度？</strong></p>
<p><strong>解答：</strong></p>
<p><strong>2. 对于下图(3-1)所示，根据你的理解Sparse Feature中的不同颜色节点分别表示什么意思?</strong> </p>
<p><strong>解答：</strong> </p>
<p>一个圆点是一个神经元 ，每个field只有一个神经元标记为黄色，可以记作1，其他记作0，field到embedding voctor的过程中只有黄色（1）这个是起作用的。可以理解为 embedding voctor （嵌入向量的具体值）是该神经元（标记为黄色）相连的5条线的权重。我也是这么理解的。</p>
<h2 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h2><ol>
<li><a href="https://blog.csdn.net/maqunfi/article/details/99635620" target="_blank" rel="noopener">https://blog.csdn.net/maqunfi/article/details/99635620</a></li>
<li><a href="https://blog.csdn.net/weixin_45459911/article/details/105359982" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45459911/article/details/105359982</a></li>
<li>王喆 《深度学习推荐系统》</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>深度推荐系统作业：wide&amp;deep介绍和代码实现</title>
    <url>/2021/03/19/%E7%9A%84/</url>
    <content><![CDATA[<h2 id="阅读前思考"><a href="#阅读前思考" class="headerlink" title="阅读前思考"></a>阅读前思考</h2><blockquote>
<ol>
<li>在你的应用场景中，哪些特征适合放在Wide侧，哪些特征适合放在Deep侧，为什么呢？</li>
<li>为什么Wide部分要用L1 FTRL训练？</li>
<li>为什么Deep部分不特别考虑稀疏性的问题？</li>
</ol>
</blockquote>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Wide&amp;Deep推荐算法出自一篇论文《Wide&amp;Deep Learning for RecommenderSystems》，Wide&amp;Deep由两部分组成，分别是Wide和Deep。Wide&amp;Deep 模型同时具备较强的 “记忆能力” 和“泛化能力”，模型能够从历史数据中学习到高频共现的特征组合的能力，称为是模型的Memorization。能够利用特征之间的传递性去探索历史数据中从未出现过的特征组合，称为是模型的Generalization。Wide&amp;Deep兼顾Memorization与Generalization并在Google Play store的场景中成功落地。</p>
<p>先来说wide，表示的是generalized的推荐系统，传统的推荐系统都是通过线性算法基于离散特征来做推荐的。</p>
<h2 id="Wide-amp-Deep原理"><a href="#Wide-amp-Deep原理" class="headerlink" title="Wide &amp; Deep原理"></a>Wide &amp; Deep原理</h2><p>首先我们来看下业内的常用的模型的结构图：</p>
<div align="center"><br><img src="http://ryluo.oss-cn-chengdu.aliyuncs.com/Javaimage-20200910214310877.png" alt="image-20200910214310877" style="zoom:65%;"><br></div>

<p>这张图源于论文，从左到右分别展示了Wide模型，Wide &amp; Deep模型以及Deep模型。从图上我们也看得出来所谓的Wide模型呢其实就是线性模型，Deep模型是深度神经网络模型。下面结合这张图对这两个部分做一个详细一点的介绍。</p>
<h3 id="Wide部分"><a href="#Wide部分" class="headerlink" title="Wide部分"></a><strong>Wide部分</strong></h3><p>Wide部分就是一个广义的线性模型，表示为y=WX+b。X特征部分包括基础特征和交叉特征(cross-product transformation)。交叉特征在wide部分很重要，可以捕捉到特征间的交互，起到添加非线性的作用。交叉特征可表示为：</p>
<p>$$\phi_{k}(x)=\prod_{i=1}^d x_i^{c_{ki}}, c_{ki}\in {0,1}<br>$$</p>
<p>$c_{ki}是一个布尔变量，当第个特征属于第个特征组合时，是一个布尔变量，当第i个特征属于第k个特征组合时，c_{ki}的值为，否则为，的值为1，否则为0，x_i$是第i个特征的值，大体意思就是两个特征都同时为1这个新的特征才能为1，否则就是0，说白了就是一个特征组合。用原论文的例子举例：</p>
<blockquote>
<p>AND(user_installed_app=QQ, impression_app=WeChat)，当特征user_installed_app=QQ,和特征impression_app=WeChat取值都为1的时候，组合特征AND(user_installed_app=QQ, impression_app=WeChat)的取值才为1，否则为0。</p>
</blockquote>
<p>对于wide部分训练时候使用的优化器是带L_1正则的FTRL算法(Follow-the-regularized-leader)，而L1 FTLR是非常注重模型稀疏性质的，也就是说W&amp;D模型采用L1 FTRL是想让Wide部分变得更加的稀疏，即Wide部分的大部分参数都为0，这就大大压缩了模型权重及特征向量的维度。<strong>Wide部分模型训练完之后留下来的特征都是非常重要的，那么模型的“记忆能力”就可以理解为发现”直接的”，“暴力的”，“显然的”关联规则的能力。</strong>例如Google W&amp;D期望wide部分发现这样的规则：<strong>用户安装了应用A，此时曝光应用B，用户安装应用B的概率大。</strong></p>
<p><strong>优点：</strong> 模型结构简单，具有较强的“记忆能力” （memorization） ，可直接学习并利用历史数据中物品或者特征的“共现频率”的能力，原始数据往往可以直接影响推荐结果。</p>
<p><strong>缺点：</strong>依赖人工特征工程,需要人工经验，这种推荐只对用户操作过的商品有效。</p>
<h2 id="Deep部分"><a href="#Deep部分" class="headerlink" title="Deep部分"></a><strong>Deep部分</strong></h2><p>Deep部分就是个前馈网络模型(深度神经网络模型)。通过深度学习学习出来的一些向量，这些向量是隐性特征，往往是没有明确可解释性的。这些向量也可以作为特征的一部分参与到训练中。这部分特征是深度学习框架自动生成的，无需人力干预。<br>特征首先转换为低维稠密向量，维度通常O（10）-O（100）。向量随机初始化，经过最小化随时函数训练模型。激活函数采用Relu。前馈部分表示如下：<br>公式<br>​<br>$$a^{(l+1)} = f(W^{l}a^{(l)} + b^{l})$$</p>
<p><strong>优点：</strong> 模型具有 “泛化能力” generalization ，找到不易于发现的规律</p>
<p><strong>缺点：</strong>不易于解释，计算量大</p>
<h2 id="Wide-amp-Deep联合训练"><a href="#Wide-amp-Deep联合训练" class="headerlink" title="Wide&amp;Deep联合训练"></a><strong>Wide&amp;Deep联合训练</strong></h2><p>在联合模型中，Wide和Deep部分的输出通过加权方式合并到一起，并通过logistic loss function进行最终输出。</p>
<p>联合训练和模型集成要进行区分，他们有着以下两点区别：</p>
<p>训练方式。 集成模型的子模型部分是独立训练，只在inference阶段合并预测。而联合训练模型是同时训练同时产出的。<br>模型规模。集成模型独立训练，模型规模要大一些才能达到可接受的效果。而联合训练模型中，Wide部分只需补充Deep模型的缺点，即记忆能力，这部分主要通过小规模的交叉特征实现。因此联合训练模型的Wide部分的模型特征较小。<br>联合模型求解采用FTRL算法，L1正则。深度部分用AdaGrad优化算法。</p>
<p><strong>优点：</strong>同时学习低阶和高阶组合特征，它混合了一个线性模型（Wide part）和Deep模型(Deep part)，能够快速处理并记忆大量历史行为特征，并<br>且具有强大的表达能力。</p>
<p><strong>缺点：</strong>这两部分模型需要不同的输入，而Wide part部分的输入，依旧依赖人工特征工程。</p>
<h2 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h2><p>完整数据集:<a href="https://labs.criteo.com/2014/02/download-kaggle-display-advertising-challenge-dataset/" target="_blank" rel="noopener">crite</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="comment"># Author:Jo Choi</span></span><br><span class="line"><span class="comment"># Date:2021-03-18</span></span><br><span class="line"><span class="comment"># Email:cai_oo@sina.com.cn</span></span><br><span class="line"><span class="comment"># Blog: *</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">数据集：criteo_sample</span></span><br><span class="line"><span class="string">------------------------------</span></span><br><span class="line"><span class="string">运行结果：</span></span><br><span class="line"><span class="string">----------------------------</span></span><br><span class="line"><span class="string">loss: 0.4625 - binary_crossentropy: 0.4625 - auc: 0.822 - 0s 36ms/step - loss: 0.4164 - </span></span><br><span class="line"><span class="string">binary_crossentropy: 0.4164 - auc: 0.8191 - val_loss: 0.6470 - </span></span><br><span class="line"><span class="string">val_binary_crossentropy: 0.6470 - val_auc: 0.6595</span></span><br><span class="line"><span class="string">----------------------------</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler,LabelEncoder</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> SparseFeat, DenseFeat, VarLenSparseFeat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_process</span><span class="params">(data_df,dense_features,sparse_features)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    数据预处理，包括填充缺失值，数值处理，类别编码</span></span><br><span class="line"><span class="string">    :param data_df: Data_Frame格式的数据</span></span><br><span class="line"><span class="string">    :param dense_features: 数值特征名称列表</span></span><br><span class="line"><span class="string">    :param sparse_features: 离散特征名称列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">     <span class="comment">#数值型特征缺失值填充0.0</span></span><br><span class="line">    data_df[dense_features] = data_df[dense_features].fillna(<span class="number">0.0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> dense_features:</span><br><span class="line">        data_df[f] = data_df[f].apply(<span class="keyword">lambda</span> x: np.log(x + <span class="number">1</span>) <span class="keyword">if</span> x &gt; <span class="number">-1</span> <span class="keyword">else</span> <span class="number">-1</span>)</span><br><span class="line">     </span><br><span class="line">    <span class="comment">#离散型特征缺失值填充-1   </span></span><br><span class="line">    data_df[sparse_features] = data_df[sparse_features].fillna(<span class="string">"-1"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> sparse_features:</span><br><span class="line">        <span class="comment">#标准化</span></span><br><span class="line">        lbe = LabelEncoder()</span><br><span class="line">        data_df[f] = lbe.fit_transform(data_df[f])</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#返回    </span></span><br><span class="line">    <span class="keyword">return</span> data_df[dense_features + sparse_features]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_input_layers</span><span class="params">(feature_columns)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    构建输入层</span></span><br><span class="line"><span class="string">    :param feature_columns : 数据集中的所有特征对应的特征标记</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建input 层字典，并以dense 和 sparse 两类字典的形式返回</span></span><br><span class="line">    dense_input_dict,sparse_input_dict = &#123;&#125; ,&#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> feature_columns:</span><br><span class="line">        <span class="keyword">if</span> isinstance(fc,SparseFeat):</span><br><span class="line">            sparse_input_dict[fc.name] = Input(shape = (<span class="number">1</span>,), name = fc.name)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(fc,DenseFeat):</span><br><span class="line">            dense_input_dict[fc.name] = Input(shape = (fc.dimension, ), name = fc.name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dense_input_dict, sparse_input_dict</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_embedding_layers</span><span class="params">(feature_columns, input_layers_dict, is_linear)</span>:</span></span><br><span class="line">    <span class="comment"># 定义一个embedding层对应的字典</span></span><br><span class="line">    embedding_layers_dict = dict()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将特征中的sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), feature_columns)) <span class="keyword">if</span> feature_columns <span class="keyword">else</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果是用于线性部分的embedding层，其维度为1，否则维度就是自己定义的embedding维度</span></span><br><span class="line">    <span class="keyword">if</span> is_linear:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, <span class="number">1</span>, name = <span class="string">'1d_emb_'</span> + fc.name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">            embedding_layers_dict[fc.name] = Embedding(fc.vocabulary_size + <span class="number">1</span>, fc.embedding_dim , name = <span class="string">'kd_emb_'</span> + fc.name) </span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> embedding_layers_dict </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_linear_logits</span><span class="params">(dense_input_dict, sparse_input_dict, sparse_feature_columns)</span>:</span></span><br><span class="line">    <span class="comment"># 将所有的dense特征的Input层，然后经过一个全连接层得到dense特征的logits</span></span><br><span class="line">    concat_dense_inputs = Concatenate(axis=<span class="number">1</span>)(list(dense_input_dict.values()))</span><br><span class="line">    dense_logits_output = Dense(<span class="number">1</span>)(concat_dense_inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取linear部分sparse特征的embedding层，这里使用embedding的原因是：</span></span><br><span class="line">    <span class="comment"># 对于linear部分直接将特征进行onehot然后通过一个全连接层，当维度特别大的时候，计算比较慢</span></span><br><span class="line">    <span class="comment"># 使用embedding层的好处就是可以通过查表的方式获取到哪些非零的元素对应的权重，然后在将这些权重相加，效率比较高</span></span><br><span class="line">    linear_embedding_layers = build_embedding_layers(sparse_feature_columns, sparse_input_dict, is_linear=<span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将一维的embedding拼接，注意这里需要使用一个Flatten层，使维度对应</span></span><br><span class="line">    sparse_1d_embed = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        feat_input = sparse_input_dict[fc.name]</span><br><span class="line">        embed = Flatten()(linear_embedding_layers[fc.name](feat_input)) <span class="comment"># B x 1</span></span><br><span class="line">        sparse_1d_embed.append(embed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># embedding中查询得到的权重就是对应onehot向量中一个位置的权重，所以后面不用再接一个全连接了，本身一维的embedding就相当于全连接</span></span><br><span class="line">    <span class="comment"># 只不过是这里的输入特征只有0和1，所以直接向非零元素对应的权重相加就等同于进行了全连接操作(非零元素部分乘的是1)</span></span><br><span class="line">    sparse_logits_output = Add()(sparse_1d_embed)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最终将dense特征和sparse特征对应的logits相加，得到最终linear的logits</span></span><br><span class="line">    linear_logits = Add()([dense_logits_output, sparse_logits_output])</span><br><span class="line">    <span class="keyword">return</span> linear_logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将所有的sparse特征embedding拼接</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">concat_embedding_list</span><span class="params">(feature_columns, input_layer_dict, embedding_layer_dict, flatten = False)</span>:</span></span><br><span class="line">    <span class="comment"># 将sparse特征筛选出来</span></span><br><span class="line">    sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), feature_columns))</span><br><span class="line">    </span><br><span class="line">    embedding_list = []</span><br><span class="line">    <span class="keyword">for</span> fc <span class="keyword">in</span> sparse_feature_columns:</span><br><span class="line">        <span class="comment"># 获取输入层</span></span><br><span class="line">        _input = input_layer_dict[fc.name]</span><br><span class="line">        <span class="comment"># B x 1 x dim 获取对应的embedding层</span></span><br><span class="line">        _embed = embedding_layer_dict[fc.name]</span><br><span class="line">        <span class="comment"># B x dim 将input层输入到embedding层中</span></span><br><span class="line">        embed = _embed(_input)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否需要flatten , 如果embedding列表最终是直接输入到Dense层中，需要进行Flatten,否则不需要</span></span><br><span class="line">        <span class="keyword">if</span> flatten:</span><br><span class="line">            embed = Flatten()(embed)</span><br><span class="line">        </span><br><span class="line">        embedding_list.append(embed)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> embedding_list </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dnn_logits</span><span class="params">(dense_input_dict, sparse_input_dict, sparse_feature_columns, dnn_embedding_layers)</span>:</span></span><br><span class="line">    concat_dense_inputs = Concatenate(axis=<span class="number">1</span>)(list(dense_input_dict.values())) <span class="comment"># B x n1 (n表示的是dense特征的维度) </span></span><br><span class="line"></span><br><span class="line">    sparse_kd_embed = concat_embedding_list(sparse_feature_columns, sparse_input_dict, dnn_embedding_layers, flatten=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    concat_sparse_kd_embed = Concatenate(axis=<span class="number">1</span>)(sparse_kd_embed) <span class="comment"># B x n2k  (n2表示的是Sparse特征的维度)</span></span><br><span class="line"></span><br><span class="line">    dnn_input = Concatenate(axis=<span class="number">1</span>)([concat_dense_inputs, concat_sparse_kd_embed]) <span class="comment"># B x (n2k + n1)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># dnn层，这里的Dropout参数，Dense中的参数及Dense的层数都可以自己设定</span></span><br><span class="line">    dnn_out = Dropout(<span class="number">0.5</span>)(Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>)(dnn_input))  </span><br><span class="line">    dnn_out = Dropout(<span class="number">0.3</span>)(Dense(<span class="number">512</span>, activation=<span class="string">'relu'</span>)(dnn_out))</span><br><span class="line">    dnn_out = Dropout(<span class="number">0.1</span>)(Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>)(dnn_out))</span><br><span class="line"></span><br><span class="line">    dnn_logits = Dense(<span class="number">1</span>)(dnn_out)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dnn_logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wide&amp;Deep 模型的wide部分及Deep部分的特征选择，应该根据实际的业务场景去确定哪些特征应该放在Wide部分，哪些特征应该放在Deep部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">WideNDeep</span><span class="params">(linear_feature_columns, dnn_feature_columns)</span>:</span></span><br><span class="line">    <span class="comment"># 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span></span><br><span class="line">    dense_input_dict, sparse_input_dict = build_input_layers(linear_feature_columns + dnn_feature_columns)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将linear部分的特征中sparse特征筛选出来，后面用来做1维的embedding</span></span><br><span class="line">    linear_sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), linear_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span></span><br><span class="line">    <span class="comment"># 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span></span><br><span class="line">    input_layers = list(dense_input_dict.values()) + list(sparse_input_dict.values())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Wide&amp;Deep模型论文中Wide部分使用的特征比较简单，并且得到的特征非常的稀疏，所以使用了FTRL优化Wide部分（这里没有实现FTRL）</span></span><br><span class="line">    <span class="comment"># 但是是根据他们业务进行选择的，我们这里将所有可能用到的特征都输入到Wide部分，具体的细节可以根据需求进行修改</span></span><br><span class="line">    linear_logits = get_linear_logits(dense_input_dict, sparse_input_dict, linear_sparse_feature_columns)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span></span><br><span class="line">    embedding_layers = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    dnn_sparse_feature_columns = list(filter(<span class="keyword">lambda</span> x: isinstance(x, SparseFeat), dnn_feature_columns))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在Wide&amp;Deep模型中，deep部分的输入是将dense特征和embedding特征拼在一起输入到dnn中</span></span><br><span class="line">    dnn_logits = get_dnn_logits(dense_input_dict, sparse_input_dict, dnn_sparse_feature_columns, embedding_layers)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将linear,dnn的logits相加作为最终的logits</span></span><br><span class="line">    output_logits = Add()([linear_logits, dnn_logits])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的激活函数使用sigmoid</span></span><br><span class="line">    output_layer = Activation(<span class="string">"sigmoid"</span>)(output_logits)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layer)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 读取数据</span></span><br><span class="line">    data = pd.read_csv(<span class="string">'./data/criteo_sample.txt'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 划分dense和sparse特征</span></span><br><span class="line">    columns = data.columns.values</span><br><span class="line">    dense_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'I'</span> <span class="keyword">in</span> feat]</span><br><span class="line">    sparse_features = [feat <span class="keyword">for</span> feat <span class="keyword">in</span> columns <span class="keyword">if</span> <span class="string">'C'</span> <span class="keyword">in</span> feat]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 简单的数据预处理</span></span><br><span class="line">    train_data = data_process(data, dense_features, sparse_features)</span><br><span class="line">    train_data[<span class="string">'label'</span>] = data[<span class="string">'label'</span>]</span><br><span class="line">    </span><br><span class="line">  <span class="comment"># 将特征分组，分成linear部分和dnn部分(根据实际场景进行选择)，并将分组之后的特征做标记（使用DenseFeat, SparseFeat）</span></span><br><span class="line">    linear_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">    dnn_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(),embedding_dim=<span class="number">4</span>)</span><br><span class="line">                            <span class="keyword">for</span> i,feat <span class="keyword">in</span> enumerate(sparse_features)] + [DenseFeat(feat, <span class="number">1</span>,)</span><br><span class="line">                            <span class="keyword">for</span> feat <span class="keyword">in</span> dense_features]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建WideNDeep模型</span></span><br><span class="line">    history = WideNDeep(linear_feature_columns, dnn_feature_columns)</span><br><span class="line">    history.summary()</span><br><span class="line">    history.compile(optimizer=<span class="string">"adam"</span>, </span><br><span class="line">                loss=<span class="string">"binary_crossentropy"</span>, </span><br><span class="line">                metrics=[<span class="string">"binary_crossentropy"</span>, tf.keras.metrics.AUC(name=<span class="string">'auc'</span>)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入数据转化成字典的形式输入</span></span><br><span class="line">    train_model_input = &#123;name: data[name] <span class="keyword">for</span> name <span class="keyword">in</span> dense_features + sparse_features&#125;</span><br><span class="line">    <span class="comment"># 模型训练</span></span><br><span class="line">    history.fit(train_model_input, train_data[<span class="string">'label'</span>].values,</span><br><span class="line">            batch_size=<span class="number">64</span>, epochs=<span class="number">8</span>, validation_split=<span class="number">0.2</span>, )</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Model: &quot;functional_3&quot;</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Layer (type)                    Output Shape         Param #     Connected to                     </span><br><span class="line">==================================================================================================</span><br><span class="line">C1 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C2 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C3 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C4 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C5 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C6 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C7 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C8 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C9 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C10 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C11 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C12 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C13 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C14 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C15 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C16 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C17 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C18 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C19 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C20 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C21 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C22 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C23 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C24 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C25 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">C26 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C1 (Embedding)           (None, 1, 4)         112         C1[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C2 (Embedding)           (None, 1, 4)         372         C2[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C3 (Embedding)           (None, 1, 4)         692         C3[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C4 (Embedding)           (None, 1, 4)         632         C4[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C5 (Embedding)           (None, 1, 4)         52          C5[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C6 (Embedding)           (None, 1, 4)         32          C6[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C7 (Embedding)           (None, 1, 4)         736         C7[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C8 (Embedding)           (None, 1, 4)         80          C8[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C9 (Embedding)           (None, 1, 4)         12          C9[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C10 (Embedding)          (None, 1, 4)         572         C10[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C11 (Embedding)          (None, 1, 4)         696         C11[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C12 (Embedding)          (None, 1, 4)         684         C12[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C13 (Embedding)          (None, 1, 4)         668         C13[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C14 (Embedding)          (None, 1, 4)         60          C14[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C15 (Embedding)          (None, 1, 4)         684         C15[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C16 (Embedding)          (None, 1, 4)         676         C16[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C17 (Embedding)          (None, 1, 4)         40          C17[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C18 (Embedding)          (None, 1, 4)         512         C18[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C19 (Embedding)          (None, 1, 4)         180         C19[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C20 (Embedding)          (None, 1, 4)         20          C20[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C21 (Embedding)          (None, 1, 4)         680         C21[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C22 (Embedding)          (None, 1, 4)         28          C22[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C23 (Embedding)          (None, 1, 4)         44          C23[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C24 (Embedding)          (None, 1, 4)         504         C24[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C25 (Embedding)          (None, 1, 4)         84          C25[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">kd_emb_C26 (Embedding)          (None, 1, 4)         364         C26[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I1 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I2 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I3 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I4 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I5 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I6 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I7 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I8 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I9 (InputLayer)                 [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I10 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I11 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I12 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">I13 (InputLayer)                [(None, 1)]          0                                            </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_52 (Flatten)            (None, 4)            0           kd_emb_C1[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_53 (Flatten)            (None, 4)            0           kd_emb_C2[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_54 (Flatten)            (None, 4)            0           kd_emb_C3[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_55 (Flatten)            (None, 4)            0           kd_emb_C4[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_56 (Flatten)            (None, 4)            0           kd_emb_C5[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_57 (Flatten)            (None, 4)            0           kd_emb_C6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_58 (Flatten)            (None, 4)            0           kd_emb_C7[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_59 (Flatten)            (None, 4)            0           kd_emb_C8[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_60 (Flatten)            (None, 4)            0           kd_emb_C9[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_61 (Flatten)            (None, 4)            0           kd_emb_C10[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_62 (Flatten)            (None, 4)            0           kd_emb_C11[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_63 (Flatten)            (None, 4)            0           kd_emb_C12[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_64 (Flatten)            (None, 4)            0           kd_emb_C13[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_65 (Flatten)            (None, 4)            0           kd_emb_C14[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_66 (Flatten)            (None, 4)            0           kd_emb_C15[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_67 (Flatten)            (None, 4)            0           kd_emb_C16[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_68 (Flatten)            (None, 4)            0           kd_emb_C17[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_69 (Flatten)            (None, 4)            0           kd_emb_C18[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_70 (Flatten)            (None, 4)            0           kd_emb_C19[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_71 (Flatten)            (None, 4)            0           kd_emb_C20[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_72 (Flatten)            (None, 4)            0           kd_emb_C21[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_73 (Flatten)            (None, 4)            0           kd_emb_C22[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_74 (Flatten)            (None, 4)            0           kd_emb_C23[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_75 (Flatten)            (None, 4)            0           kd_emb_C24[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_76 (Flatten)            (None, 4)            0           kd_emb_C25[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_77 (Flatten)            (None, 4)            0           kd_emb_C26[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">concatenate_4 (Concatenate)     (None, 13)           0           I1[0][0]                         </span><br><span class="line">                                                                 I2[0][0]                         </span><br><span class="line">                                                                 I3[0][0]                         </span><br><span class="line">                                                                 I4[0][0]                         </span><br><span class="line">                                                                 I5[0][0]                         </span><br><span class="line">                                                                 I6[0][0]                         </span><br><span class="line">                                                                 I7[0][0]                         </span><br><span class="line">                                                                 I8[0][0]                         </span><br><span class="line">                                                                 I9[0][0]                         </span><br><span class="line">                                                                 I10[0][0]                        </span><br><span class="line">                                                                 I11[0][0]                        </span><br><span class="line">                                                                 I12[0][0]                        </span><br><span class="line">                                                                 I13[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">concatenate_5 (Concatenate)     (None, 104)          0           flatten_52[0][0]                 </span><br><span class="line">                                                                 flatten_53[0][0]                 </span><br><span class="line">                                                                 flatten_54[0][0]                 </span><br><span class="line">                                                                 flatten_55[0][0]                 </span><br><span class="line">                                                                 flatten_56[0][0]                 </span><br><span class="line">                                                                 flatten_57[0][0]                 </span><br><span class="line">                                                                 flatten_58[0][0]                 </span><br><span class="line">                                                                 flatten_59[0][0]                 </span><br><span class="line">                                                                 flatten_60[0][0]                 </span><br><span class="line">                                                                 flatten_61[0][0]                 </span><br><span class="line">                                                                 flatten_62[0][0]                 </span><br><span class="line">                                                                 flatten_63[0][0]                 </span><br><span class="line">                                                                 flatten_64[0][0]                 </span><br><span class="line">                                                                 flatten_65[0][0]                 </span><br><span class="line">                                                                 flatten_66[0][0]                 </span><br><span class="line">                                                                 flatten_67[0][0]                 </span><br><span class="line">                                                                 flatten_68[0][0]                 </span><br><span class="line">                                                                 flatten_69[0][0]                 </span><br><span class="line">                                                                 flatten_70[0][0]                 </span><br><span class="line">                                                                 flatten_71[0][0]                 </span><br><span class="line">                                                                 flatten_72[0][0]                 </span><br><span class="line">                                                                 flatten_73[0][0]                 </span><br><span class="line">                                                                 flatten_74[0][0]                 </span><br><span class="line">                                                                 flatten_75[0][0]                 </span><br><span class="line">                                                                 flatten_76[0][0]                 </span><br><span class="line">                                                                 flatten_77[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">concatenate_6 (Concatenate)     (None, 117)          0           concatenate_4[0][0]              </span><br><span class="line">                                                                 concatenate_5[0][0]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_2 (Dense)                 (None, 1024)         120832      concatenate_6[0][0]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout (Dropout)               (None, 1024)         0           dense_2[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_3 (Dense)                 (None, 512)          524800      dropout[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C1 (Embedding)           (None, 1, 1)         28          C1[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C2 (Embedding)           (None, 1, 1)         93          C2[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C3 (Embedding)           (None, 1, 1)         173         C3[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C4 (Embedding)           (None, 1, 1)         158         C4[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C5 (Embedding)           (None, 1, 1)         13          C5[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C6 (Embedding)           (None, 1, 1)         8           C6[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C7 (Embedding)           (None, 1, 1)         184         C7[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C8 (Embedding)           (None, 1, 1)         20          C8[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C9 (Embedding)           (None, 1, 1)         3           C9[0][0]                         </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C10 (Embedding)          (None, 1, 1)         143         C10[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C11 (Embedding)          (None, 1, 1)         174         C11[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C12 (Embedding)          (None, 1, 1)         171         C12[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C13 (Embedding)          (None, 1, 1)         167         C13[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C14 (Embedding)          (None, 1, 1)         15          C14[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C15 (Embedding)          (None, 1, 1)         171         C15[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C16 (Embedding)          (None, 1, 1)         169         C16[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C17 (Embedding)          (None, 1, 1)         10          C17[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C18 (Embedding)          (None, 1, 1)         128         C18[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C19 (Embedding)          (None, 1, 1)         45          C19[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C20 (Embedding)          (None, 1, 1)         5           C20[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C21 (Embedding)          (None, 1, 1)         170         C21[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C22 (Embedding)          (None, 1, 1)         7           C22[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C23 (Embedding)          (None, 1, 1)         11          C23[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C24 (Embedding)          (None, 1, 1)         126         C24[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C25 (Embedding)          (None, 1, 1)         21          C25[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">1d_emb_C26 (Embedding)          (None, 1, 1)         91          C26[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_1 (Dropout)             (None, 512)          0           dense_3[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">concatenate_3 (Concatenate)     (None, 13)           0           I1[0][0]                         </span><br><span class="line">                                                                 I2[0][0]                         </span><br><span class="line">                                                                 I3[0][0]                         </span><br><span class="line">                                                                 I4[0][0]                         </span><br><span class="line">                                                                 I5[0][0]                         </span><br><span class="line">                                                                 I6[0][0]                         </span><br><span class="line">                                                                 I7[0][0]                         </span><br><span class="line">                                                                 I8[0][0]                         </span><br><span class="line">                                                                 I9[0][0]                         </span><br><span class="line">                                                                 I10[0][0]                        </span><br><span class="line">                                                                 I11[0][0]                        </span><br><span class="line">                                                                 I12[0][0]                        </span><br><span class="line">                                                                 I13[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_26 (Flatten)            (None, 1)            0           1d_emb_C1[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_27 (Flatten)            (None, 1)            0           1d_emb_C2[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_28 (Flatten)            (None, 1)            0           1d_emb_C3[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_29 (Flatten)            (None, 1)            0           1d_emb_C4[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_30 (Flatten)            (None, 1)            0           1d_emb_C5[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_31 (Flatten)            (None, 1)            0           1d_emb_C6[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_32 (Flatten)            (None, 1)            0           1d_emb_C7[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_33 (Flatten)            (None, 1)            0           1d_emb_C8[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_34 (Flatten)            (None, 1)            0           1d_emb_C9[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_35 (Flatten)            (None, 1)            0           1d_emb_C10[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_36 (Flatten)            (None, 1)            0           1d_emb_C11[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_37 (Flatten)            (None, 1)            0           1d_emb_C12[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_38 (Flatten)            (None, 1)            0           1d_emb_C13[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_39 (Flatten)            (None, 1)            0           1d_emb_C14[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_40 (Flatten)            (None, 1)            0           1d_emb_C15[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_41 (Flatten)            (None, 1)            0           1d_emb_C16[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_42 (Flatten)            (None, 1)            0           1d_emb_C17[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_43 (Flatten)            (None, 1)            0           1d_emb_C18[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_44 (Flatten)            (None, 1)            0           1d_emb_C19[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_45 (Flatten)            (None, 1)            0           1d_emb_C20[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_46 (Flatten)            (None, 1)            0           1d_emb_C21[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_47 (Flatten)            (None, 1)            0           1d_emb_C22[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_48 (Flatten)            (None, 1)            0           1d_emb_C23[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_49 (Flatten)            (None, 1)            0           1d_emb_C24[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_50 (Flatten)            (None, 1)            0           1d_emb_C25[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">flatten_51 (Flatten)            (None, 1)            0           1d_emb_C26[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_4 (Dense)                 (None, 256)          131328      dropout_1[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_1 (Dense)                 (None, 1)            14          concatenate_3[0][0]              </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">add (Add)                       (None, 1)            0           flatten_26[0][0]                 </span><br><span class="line">                                                                 flatten_27[0][0]                 </span><br><span class="line">                                                                 flatten_28[0][0]                 </span><br><span class="line">                                                                 flatten_29[0][0]                 </span><br><span class="line">                                                                 flatten_30[0][0]                 </span><br><span class="line">                                                                 flatten_31[0][0]                 </span><br><span class="line">                                                                 flatten_32[0][0]                 </span><br><span class="line">                                                                 flatten_33[0][0]                 </span><br><span class="line">                                                                 flatten_34[0][0]                 </span><br><span class="line">                                                                 flatten_35[0][0]                 </span><br><span class="line">                                                                 flatten_36[0][0]                 </span><br><span class="line">                                                                 flatten_37[0][0]                 </span><br><span class="line">                                                                 flatten_38[0][0]                 </span><br><span class="line">                                                                 flatten_39[0][0]                 </span><br><span class="line">                                                                 flatten_40[0][0]                 </span><br><span class="line">                                                                 flatten_41[0][0]                 </span><br><span class="line">                                                                 flatten_42[0][0]                 </span><br><span class="line">                                                                 flatten_43[0][0]                 </span><br><span class="line">                                                                 flatten_44[0][0]                 </span><br><span class="line">                                                                 flatten_45[0][0]                 </span><br><span class="line">                                                                 flatten_46[0][0]                 </span><br><span class="line">                                                                 flatten_47[0][0]                 </span><br><span class="line">                                                                 flatten_48[0][0]                 </span><br><span class="line">                                                                 flatten_49[0][0]                 </span><br><span class="line">                                                                 flatten_50[0][0]                 </span><br><span class="line">                                                                 flatten_51[0][0]                 </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dropout_2 (Dropout)             (None, 256)          0           dense_4[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">add_1 (Add)                     (None, 1)            0           dense_1[0][0]                    </span><br><span class="line">                                                                 add[0][0]                        </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">dense_5 (Dense)                 (None, 1)            257         dropout_2[0][0]                  </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">add_2 (Add)                     (None, 1)            0           add_1[0][0]                      </span><br><span class="line">                                                                 dense_5[0][0]                    </span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">activation (Activation)         (None, 1)            0           add_2[0][0]                      </span><br><span class="line">==================================================================================================</span><br><span class="line">Total params: 788,751</span><br><span class="line">Trainable params: 788,751</span><br><span class="line">Non-trainable params: 0</span><br><span class="line">__________________________________________________________________________________________________</span><br><span class="line">Epoch 1/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.6686 - binary_crossentropy: 0.6686 - auc: 0.660 - 1s 322ms/step - loss: 0.8642 - binary_crossentropy: 0.8642 - auc: 0.5307 - val_loss: 0.9196 - val_binary_crossentropy: 0.9196 - val_auc: 0.6852</span><br><span class="line">Epoch 2/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.6993 - binary_crossentropy: 0.6993 - auc: 0.587 - 0s 33ms/step - loss: 0.7057 - binary_crossentropy: 0.7057 - auc: 0.5780 - val_loss: 0.6952 - val_binary_crossentropy: 0.6952 - val_auc: 0.6667</span><br><span class="line">Epoch 3/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.5635 - binary_crossentropy: 0.5635 - auc: 0.688 - 0s 32ms/step - loss: 0.6362 - binary_crossentropy: 0.6362 - auc: 0.5944 - val_loss: 0.6602 - val_binary_crossentropy: 0.6602 - val_auc: 0.6880</span><br><span class="line">Epoch 4/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.6217 - binary_crossentropy: 0.6217 - auc: 0.591 - 0s 39ms/step - loss: 0.6272 - binary_crossentropy: 0.6272 - auc: 0.5453 - val_loss: 0.6744 - val_binary_crossentropy: 0.6744 - val_auc: 0.7464</span><br><span class="line">Epoch 5/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.5482 - binary_crossentropy: 0.5482 - auc: 0.564 - 0s 34ms/step - loss: 0.5380 - binary_crossentropy: 0.5380 - auc: 0.6284 - val_loss: 0.6426 - val_binary_crossentropy: 0.6426 - val_auc: 0.7265</span><br><span class="line">Epoch 6/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.5465 - binary_crossentropy: 0.5465 - auc: 0.635 - 0s 31ms/step - loss: 0.4800 - binary_crossentropy: 0.4800 - auc: 0.7146 - val_loss: 0.6307 - val_binary_crossentropy: 0.6307 - val_auc: 0.6652</span><br><span class="line">Epoch 7/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.5032 - binary_crossentropy: 0.5032 - auc: 0.699 - 0s 30ms/step - loss: 0.4948 - binary_crossentropy: 0.4948 - auc: 0.6784 - val_loss: 0.6486 - val_binary_crossentropy: 0.6486 - val_auc: 0.6524</span><br><span class="line">Epoch 8/8</span><br><span class="line">3/3 [==============================] - ETA: 0s - loss: 0.4625 - binary_crossentropy: 0.4625 - auc: 0.822 - 0s 36ms/step - loss: 0.4164 - binary_crossentropy: 0.4164 - auc: 0.8191 - val_loss: 0.6470 - val_binary_crossentropy: 0.6470 - val_auc: 0.6595</span><br></pre></td></tr></table></figure>
<h4 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h4><p>1） <a href="https://www.jianshu.com/p/e73146094478" target="_blank" rel="noopener">https://www.jianshu.com/p/e73146094478</a></p>
<p>2） <a href="https://zhuanlan.zhihu.com/p/57247478" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57247478</a></p>
<p>3） 王喆 《深度学习推荐系统》</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
  </entry>
</search>
